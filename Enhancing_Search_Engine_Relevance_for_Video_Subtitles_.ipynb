{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFicRQd+UZNjDfGgatDWPJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtp6EqH65mDo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "\n",
        "conn = sqlite3.connect(r'/content/drive/MyDrive/Innomatics/Copy of eng_subtitles_database.db')\n",
        "df = pd.read_sql_query(\"SELECT num,name,content FROM zipfiles LIMIT 100\", conn)\n",
        "def decode_method(binary_data):\n",
        "    \"\"\"Extracts and decodes subtitle content from a compressed binary file.\"\"\"\n",
        "    try:\n",
        "        with io.BytesIO(binary_data) as f:\n",
        "            with zipfile.ZipFile(f, 'r') as zip_file:\n",
        "                file_name = zip_file.namelist()[0]\n",
        "                subtitle_content = zip_file.read(file_name)\n",
        "                return subtitle_content.decode('utf-8', errors='replace')\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding ZIP file: {e}\")\n",
        "        return None\n",
        "df['file_content'] = df['content'].apply(decode_method)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans subtitle text by removing timestamps, numbers, and unwanted characters.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"cleaned_text\"] = df[\"file_content\"].apply(clean_text)\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/Innomatics/cleaned_subtitles.csv\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "df[[\"num\",\"name\", \"cleaned_text\"]].to_csv(output_path, index=False)\n",
        "print(f\"Processed subtitle data saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "2qAEHwjn8tPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install langchain_huggingface\n",
        "!pip install langchain_chroma\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade --force-reinstall sentence-transformers transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "35CORQEy_jc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "\n",
        "CHROMA_DB_PATH = \"/content/drive/MyDrive/Innomatics/chroma_db\"\n",
        "\n",
        "\n",
        "# Load processed subtitle data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Innomatics/cleaned_subtitles.csv\")\n",
        "\n",
        "# Initialize text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "# Prepare chunks for embedding\n",
        "chunks = []\n",
        "for _, row in df.iterrows():\n",
        "    num = str(row[\"num\"])\n",
        "    name = str(row[\"name\"])\n",
        "    text = row[\"cleaned_text\"]\n",
        "\n",
        "    if pd.isna(text) or not text.strip():\n",
        "        continue  # Skip empty texts\n",
        "\n",
        "    # Split text into smaller chunks\n",
        "    split_chunks = text_splitter.create_documents([text], metadatas=[{\"num\": num, \"name\":name}])\n",
        "    chunks.extend(split_chunks)\n",
        "\n",
        "\n",
        "def store_embeddings(chunks):\n",
        "    # Load the embedding model\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Store the embeddings in ChromaDB\n",
        "    db = Chroma.from_documents(chunks, embeddings, persist_directory=\"/content/drive/MyDrive/Innomatics/chroma_db\")\n",
        "\n",
        "    print(\"âœ… Embeddings stored successfully in ChromaDB!\")\n",
        "    return db\n",
        "\n",
        "\n",
        "def load_vector_store():\n",
        "    \"\"\"Load the stored ChromaDB vector store.\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "    db = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)\n",
        "    return db\n",
        "\n",
        "\n",
        "# Store embeddings in ChromaDB\n",
        "store_embeddings(chunks)\n"
      ],
      "metadata": {
        "id": "uFyXXmhL-OUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install streamlit\n",
        "!pip install assemblyai\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LVCRrR1fF1F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import assemblyai as aai\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "import tempfile\n",
        "\n",
        "# Initialize AssemblyAI\n",
        "aai.settings.api_key = \"key\"\n",
        "transcriber = aai.Transcriber()\n",
        "\n",
        "# Load embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load ChromaDB\n",
        "db = Chroma(persist_directory=\"/content/drive/MyDrive/Innomatics/chroma_db\", embedding_function=embeddings)\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Convert audio to text using AssemblyAI.\"\"\"\n",
        "    transcript = transcriber.transcribe(audio_path)\n",
        "    return transcript.text if transcript else \"\"\n",
        "\n",
        "def retrieve_highest_score_chunk(query: str):\n",
        "    \"\"\"Retrieve the highest-score document chunk from ChromaDB with metadata.\"\"\"\n",
        "    results = db.similarity_search_with_score(query, k=5)  # Retrieve top 5 results first\n",
        "    if results:\n",
        "        highest_score_doc, highest_score = max(results, key=lambda x: x[1])\n",
        "        return {\n",
        "            \"num\": highest_score_doc.metadata.get(\"num\", \"Unknown\"),\n",
        "            \"name\": highest_score_doc.metadata.get(\"name\", \"Unknown\"),\n",
        "            \"content\": highest_score_doc.page_content,\n",
        "            \"score\": highest_score  # Include similarity score\n",
        "        }\n",
        "    return None  # Return None if no results found\n",
        "\n",
        "# Streamlit UI with styling\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "        .main {background-color: #f5f7fa;}\n",
        "        .stTextArea {border-radius: 10px; background-color: #eef2f3;}\n",
        "        .stFileUploader {background-color: #d1e8ff; padding: 10px; border-radius: 10px;}\n",
        "        .stSubheader {color: #ff5733; font-size: 18px;}\n",
        "        .stWrite {color: #2ca02c; font-size: 16px;}\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "st.markdown(\"<h3 style='text-align: center;'>Enhancing Search Engine Relevance for Video Subtitles</h3>\", unsafe_allow_html=True)\n",
        "\n",
        "col1, col2 = st.columns([3, 1])\n",
        "\n",
        "with col2:\n",
        "    st.markdown('<div class=\"stFileUploader\">', unsafe_allow_html=True)\n",
        "    uploaded_file = st.file_uploader(\"Upload an MP3 file\", type=[\"mp3\"])\n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "with col1:\n",
        "    if uploaded_file is not None:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_audio:\n",
        "            temp_audio.write(uploaded_file.read())\n",
        "            temp_audio_path = temp_audio.name\n",
        "\n",
        "        st.write(\"Transcribing audio...\")\n",
        "        transcript_text = transcribe_audio(temp_audio_path)\n",
        "\n",
        "        if transcript_text:\n",
        "            st.subheader(\"Transcript\")\n",
        "            st.text_area(\"Transcription Output\", transcript_text, height=200)\n",
        "\n",
        "            st.write(\"Searching for the most relevant subtitle...\")\n",
        "            highest_chunk = retrieve_highest_score_chunk(transcript_text)\n",
        "\n",
        "            if highest_chunk:\n",
        "                st.subheader(\"Title\")\n",
        "                subtitle_name = highest_chunk['name']\n",
        "                subtitle_num = highest_chunk['num']\n",
        "                subtitle_url = f\"https://www.opensubtitles.org/en/subtitles/{subtitle_num}\"\n",
        "\n",
        "                st.write(f\"ðŸŽ¬ {subtitle_name} (Score: {highest_chunk['score']:.4f})\")\n",
        "                st.markdown(f\"[ðŸ”— View Subtitle on OpenSubtitles]({subtitle_url})\", unsafe_allow_html=True)\n",
        "            else:\n",
        "                st.write(\"No relevant subtitles found.\")\n",
        "        else:\n",
        "            st.write(\"Failed to transcribe audio.\")\n"
      ],
      "metadata": {
        "id": "tfCMahSy2-3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "KuflxpTMGy7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "g9xIyUO9N_8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}